'use strict';
var __awaiter =
  (this && this.__awaiter) ||
  function (thisArg, _arguments, P, generator) {
    function adopt(value) {
      return value instanceof P ? value : (
          new P(function (resolve) {
            resolve(value);
          })
        );
    }
    return new (P || (P = Promise))(function (resolve, reject) {
      function fulfilled(value) {
        try {
          step(generator.next(value));
        } catch (e) {
          reject(e);
        }
      }
      function rejected(value) {
        try {
          step(generator['throw'](value));
        } catch (e) {
          reject(e);
        }
      }
      function step(result) {
        result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected);
      }
      step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
  };
Object.defineProperty(exports, '__esModule', { value: true });
const tokenizer_1 = require('@anthropic-ai/tokenizer');
describe('countTokens', () => {
  test('small text', () =>
    __awaiter(void 0, void 0, void 0, function* () {
      expect((0, tokenizer_1.countTokens)('hello world!')).toEqual(3);
    }));
  test('text normalising', () => {
    expect((0, tokenizer_1.countTokens)('™')).toEqual(1);
    expect((0, tokenizer_1.countTokens)('ϰ')).toEqual(1);
  });
  test('allows special tokens', () => {
    expect((0, tokenizer_1.countTokens)('<EOT>')).toEqual(1);
  });
});
//# sourceMappingURL=tokenizers.test.js.map
